#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dotenv import load_dotenv
import openai
import pytz

# --------------------------------------------------
# 1) .env ã‚’èª­ã¿è¾¼ã‚€
# --------------------------------------------------
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SAVE_DIR       = os.getenv("SAVE_DIR")
NICKNAME       = os.getenv("NICKNAME")
PROFILE        = os.getenv("PROFILE")

AI_FEEDS       = [u.strip() for u in os.getenv("AI_FEEDS", "").split(",") if u.strip()]
CRYPTO_FEEDS   = [u.strip() for u in os.getenv("CRYPTO_FEEDS", "").split(",") if u.strip()]

if not all([OPENAI_API_KEY, SAVE_DIR, NICKNAME, PROFILE]):
    raise ValueError("å¿…è¦ãªç’°å¢ƒå¤‰æ•°ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

client = openai.OpenAI(api_key=OPENAI_API_KEY)
os.makedirs(SAVE_DIR, exist_ok=True)

KEYWORDS_AI     = ["ai", "äººå·¥çŸ¥èƒ½", "ç”Ÿæˆai", "chatgpt", "gpt", "openai"]
KEYWORDS_CRYPTO = ["ä»®æƒ³é€šè²¨", "æš—å·è³‡ç”£", "ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³", "ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³", "crypto"]

# JSTã®æ˜¨æ—¥ã€œä»Šæ—¥ã®ç¯„å›²ã‚’æ±‚ã‚ã‚‹
jst = pytz.timezone("Asia/Tokyo")
now = datetime.now(jst)
today = now.date()
yesterday = today - timedelta(days=1)

def is_recent(published_str):
    try:
        dt = datetime.strptime(published_str, "%a, %d %b %Y %H:%M:%S %z")
        dt_jst = dt.astimezone(jst)
        return yesterday <= dt_jst.date() <= today
    except Exception:
        return False

def fetch_feed(feed_urls, keywords):
    results = []
    for url in feed_urls:
        feed = feedparser.parse(url)
        for entry in feed.entries[:20]:
            title = entry.title.lower()
            summary = getattr(entry, "summary", "").lower()
            published = getattr(entry, "published", "")
            if any(k in title or k in summary for k in keywords):
                if is_recent(published):
                    results.append({
                        "title": entry.title,
                        "url": entry.link,
                        "published": published
                    })
    return results[:5]

def fetch_text(url):
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        return "\n".join(paragraphs)[:2000]
    except Exception as e:
        return f"(æœ¬æ–‡å–å¾—å¤±æ•—: {e})"

def gpt_summary(text):
    prompt = f"ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ—¥æœ¬èªã§200å­—ä»¥å†…ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n{text}"
    res = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=300,
        temperature=0.7,
    )
    return res.choices[0].message.content.strip()

def gpt_suggestion(summary):
    prompt = (
        f"ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ã«å¯¾ã—ã¦ã€{PROFILE} ã«å‘ã‘ãŸå…·ä½“çš„ãªè¡Œå‹•ææ¡ˆã‚’"
        "3ã¤ãƒªã‚¹ãƒˆå½¢å¼ã§å‡ºã—ã¦ãã ã•ã„ã€‚\n"
        "ç™ºä¿¡ã€å‰¯æ¥­ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åŒ–ã€åç›ŠåŒ–ãªã©ã«ã¤ãªãŒã‚‹ã€å®Ÿè¡Œå¯èƒ½ã§å°‘ã—æŒ‘æˆ¦çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"{summary}"
    )
    res = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500,
        temperature=0.8,
    )
    return res.choices[0].message.content.strip()

def tag_from_suggestion(suggestion):
    s = suggestion.lower()
    tags = set()
    if "ai" in s or "äººå·¥çŸ¥èƒ½" in s: tags.add("#AI")
    if any(k in s for k in ["ä»®æƒ³é€šè²¨", "ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³", "æš—å·è³‡ç”£"]): tags.add("#ä»®æƒ³é€šè²¨")
    tags.update({"#å‰¯æ¥­", "#åç›ŠåŒ–æ¡ˆ"})
    if "note"      in s: tags.add("#noteæ¡ˆ")
    if "twitter"   in s or "xã§" in s or "ãƒã‚¹ãƒˆ" in s: tags.add("#Xæ¡ˆ")
    if "youtube"   in s: tags.add("#YouTubeæ¡ˆ")
    if "kindle"    in s: tags.add("#Kindleæ¡ˆ")
    if "udemy"     in s: tags.add("#Udemyæ¡ˆ")
    if "stand.fm"  in s or "éŸ³å£°é…ä¿¡" in s: tags.add("#éŸ³å£°é…ä¿¡æ¡ˆ")
    return " ".join(sorted(tags))

def process_articles(articles, header):
    section = [f"## {header}\n"]
    for idx, art in enumerate(articles, 1):
        text       = fetch_text(art["url"])
        summary    = gpt_summary(text)
        suggestion = gpt_suggestion(summary)
        tags       = tag_from_suggestion(suggestion)

        section.append(f"### ã€”{idx}ã€• {art['title']}")
        section.append(f"- **å…¬é–‹æ—¥:** {art['published']}")
        section.append(f"- **URL:** {art['url']}\n")
        section.append(f"**è¦ç´„:** {summary}\n")
        section.append(f"**{NICKNAME}ã¸ã®ææ¡ˆ:**\n{suggestion}\n")
        section.append(f"{tags}\n")
    return section

def main():
    ai_articles = fetch_feed(AI_FEEDS, KEYWORDS_AI)
    crypto_articles = fetch_feed(CRYPTO_FEEDS, KEYWORDS_CRYPTO)

    filename = f"news_{now.strftime('%Y%m%d')}.md"
    content = [f"# {now.strftime('%Y-%m-%d')} ã® AIãƒ»ä»®æƒ³é€šè²¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚\n"]
    content += process_articles(ai_articles, "ğŸ”· AIãƒ‹ãƒ¥ãƒ¼ã‚¹")
    content += process_articles(crypto_articles, "ğŸ”¶ ä»®æƒ³é€šè²¨ãƒ‹ãƒ¥ãƒ¼ã‚¹")

    with open(os.path.join(SAVE_DIR, filename), "w", encoding="utf-8") as f:
        f.write("\n".join(content))

    print(f"âœ… ä¿å­˜å®Œäº†: {filename}")

if __name__ == "__main__":
    main()

