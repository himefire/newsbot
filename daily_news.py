#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â–¶ AIï¼ä»®æƒ³é€šè²¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è‡ªå‹•è¦ç´„ã—ã€Obsidian Vault ã«ä¿å­˜
   â”” å–å¾—å¯¾è±¡ : ã€Œå‰æ—¥ 0:00 JST ã€œ å½“æ—¥ 23:59 JSTã€ ã«å…¬é–‹ã•ã‚ŒãŸè¨˜äº‹ã®ã¿
   â”” ChatGPT ã§è¡æ’ƒåº¦ãƒ»é¢ç™½ã•é †ã« TOP5 ã‚’æŠ½å‡º
   â”” å€‹äººæƒ…å ±ã‚„ã‚­ãƒ¼é¡ã¯ .env ã§ç®¡ç†ï¼ˆ.gitignore ã«è¿½åŠ å¿…é ˆï¼‰
"""

import os, re, time, email.utils as eut
from datetime import datetime, timedelta, timezone
import pytz
import feedparser
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import openai

# --------------------------------------------------
# 0) æ—¥æœ¬æ™‚é–“ãƒ˜ãƒ«ãƒ‘
# --------------------------------------------------
JST = pytz.timezone("Asia/Tokyo")
def jst_now() -> datetime:
    return datetime.now(JST)

# --------------------------------------------------
# 1) .env èª­ã¿è¾¼ã¿
# --------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SAVE_DIR       = os.getenv("SAVE_DIR")
NICKNAME       = os.getenv("NICKNAME")
PROFILE        = os.getenv("PROFILE")
AI_FEEDS       = [u.strip() for u in os.getenv("AI_FEEDS", "").split(",") if u.strip()]
CRYPTO_FEEDS   = [u.strip() for u in os.getenv("CRYPTO_FEEDS", "").split(",") if u.strip()]

if not all([OPENAI_API_KEY, SAVE_DIR, NICKNAME, PROFILE]):
    raise ValueError("OPENAI_API_KEY / SAVE_DIR / NICKNAME / PROFILE ãŒ .env ã«ã‚ã‚Šã¾ã›ã‚“")

# --------------------------------------------------
# 2) OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# --------------------------------------------------
client = openai.OpenAI(api_key=OPENAI_API_KEY)  # openai>=1.x

# --------------------------------------------------
# 3) ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€
# --------------------------------------------------
os.makedirs(SAVE_DIR, exist_ok=True)

# --------------------------------------------------
# 4) ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
# --------------------------------------------------
KEYWORDS_AI     = ["ai", "äººå·¥çŸ¥èƒ½", "ç”Ÿæˆai", "chatgpt", "gpt", "openai"]
KEYWORDS_CRYPTO = ["ä»®æƒ³é€šè²¨", "æš—å·è³‡ç”£", "ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³", "ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³", "crypto"]

# --------------------------------------------------
# 5) æ—¥ä»˜åˆ¤å®š
# --------------------------------------------------
def is_within_today(entry, now_jst: datetime) -> bool:
    """å‰æ—¥0:00ã€œå½“æ—¥23:59(JST) ã«å…¬é–‹ã•ã‚ŒãŸã‹åˆ¤å®š"""
    pub_dt = None

    # (a) published_parsed (struct_time) ãŒã‚ã‚‹å ´åˆ
    if getattr(entry, "published_parsed", None):
        pub_dt = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=JST)
    # (b) published æ–‡å­—åˆ— â†’ RFC822 è§£æ
    elif getattr(entry, "published", None):
        try:
            tup = eut.parsedate(entry.published)
            pub_dt = datetime.fromtimestamp(time.mktime(tup), tz=JST)
        except Exception:
            return False
    else:
        return False  # æ—¥ä»˜ä¸æ˜ã¯é™¤å¤–

    start = (now_jst - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    end   =  now_jst.replace(hour=23, minute=59, second=59, microsecond=999999)
    return start <= pub_dt <= end

# --------------------------------------------------
# 6) RSS â†’ å€™è£œå–å¾— â†’ ChatGPT ãƒ©ãƒ³ã‚­ãƒ³ã‚°
# --------------------------------------------------
def fetch_top5(feed_urls, keywords, now_jst) -> list:
    candidates = []
    for url in feed_urls:
        parsed = feedparser.parse(url)
        for e in parsed.entries:
            # å¯¾è±¡æœŸé–“+ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
            ttl = e.title
            summary = getattr(e, "summary", "")
            if not is_within_today(e, now_jst):
                continue
            if not any(k in (ttl+summary).lower() for k in keywords):
                continue
            candidates.append(
                {"title": ttl,
                 "url":   e.link,
                 "summary": summary[:120],
                 "published": getattr(e, "published", "ä¸æ˜")}
            )

    if not candidates:
        return []

    # ChatGPT ã«è¡æ’ƒåº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä¾é ¼
    lst = "\n".join(f"{i+1}. {c['title']} - {c['summary']}"
                    for i, c in enumerate(candidates))
    prompt = (
        "æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä¸­ã‹ã‚‰ã€è¡æ’ƒåº¦ãƒ»é¢ç™½ã•ãŒé«˜ã„ã‚‚ã®ã‚’5ä»¶ã ã‘é¸ã³ã€"
        "ç•ªå·ã ã‘ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¿”ã—ã¦ãã ã•ã„ã€‚\n" + lst
    )
    try:
        ans = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=20,
            temperature=0.3,
        ).choices[0].message.content
        idxs = [int(x) for x in re.findall(r"\d+", ans)][:5]
        top5 = [candidates[i-1] for i in idxs if 1 <= i <= len(candidates)]
        return top5
    except Exception:
        # å¤±æ•—ã—ãŸã‚‰å…ˆé ­5ä»¶
        return candidates[:5]

# --------------------------------------------------
# 7) æœ¬æ–‡ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° & è¦ç´„ãƒ»ææ¡ˆ
# --------------------------------------------------
def fetch_text(url, limit=2000):
    try:
        html = requests.get(url, timeout=10).text
        soup = BeautifulSoup(html, "html.parser")
        text = "\n".join(p.get_text(" ", strip=True) for p in soup.find_all("p"))
        return text[:limit]
    except Exception as e:
        return f"(æœ¬æ–‡å–å¾—å¤±æ•—: {e})"

def gpt_summary(fulltext):
    prompt = f"ä»¥ä¸‹ã®è¨˜äº‹ã‚’æ—¥æœ¬èªã§200å­—ä»¥å†…ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n{fulltext}"
    return client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=350,
        temperature=0.5,
    ).choices[0].message.content.strip()

def gpt_suggestion(summary):
    prompt = (
        f"ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ã‚’èª­ã‚“ã§ã€{PROFILE} ãŒå–ã‚‹ã¹ãè¡Œå‹•ã‚’"
        "3ã¤ã€ç®‡æ¡æ›¸ãã§ææ¡ˆã—ã¦ãã ã•ã„ï¼ˆç™ºä¿¡ãƒ»å‰¯æ¥­ãƒ»åç›ŠåŒ–ãªã©ï¼‰ã€‚\n\n{summary}"
    )
    return client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=400,
        temperature=0.8,
    ).choices[0].message.content.strip()

def tag_from_suggestion(s):
    s = s.lower()
    tags = {"#å‰¯æ¥­", "#åç›ŠåŒ–æ¡ˆ"}
    if "ai" in s or "äººå·¥çŸ¥èƒ½" in s: tags.add("#AI")
    if any(k in s for k in ["ä»®æƒ³é€šè²¨", "ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³", "æš—å·è³‡ç”£"]): tags.add("#ä»®æƒ³é€šè²¨")
    if "note" in s: tags.add("#noteæ¡ˆ")
    if "twitter" in s or "ãƒã‚¹ãƒˆ" in s or "xã§" in s: tags.add("#Xæ¡ˆ")
    if "youtube" in s: tags.add("#YouTubeæ¡ˆ")
    if "kindle" in s: tags.add("#Kindleæ¡ˆ")
    if "udemy" in s: tags.add("#Udemyæ¡ˆ")
    if any(k in s for k in ["stand.fm", "éŸ³å£°é…ä¿¡"]): tags.add("#éŸ³å£°é…ä¿¡æ¡ˆ")
    return " ".join(sorted(tags))

# --------------------------------------------------
# 8) Markdown çµ„ã¿ç«‹ã¦
# --------------------------------------------------
def build_section(articles, header):
    section = [f"## {header}\n"]
    for i, art in enumerate(articles, 1):
        body  = fetch_text(art["url"])
        summ  = gpt_summary(body)
        sugg  = gpt_suggestion(summ)
        tags  = tag_from_suggestion(sugg)

        section += [
            f"### ã€”{i}ã€• {art['title']}",
            f"- **å…¬é–‹æ—¥:** {art['published']}",
            f"- **URL:** {art['url']}\n",
            f"**è¦ç´„:** {summ}\n",
            f"**{NICKNAME}ã¸ã®ææ¡ˆ:**\n{sugg}\n",
            tags + "\n"
        ]
    return section

# --------------------------------------------------
# 9) ãƒ¡ã‚¤ãƒ³
# --------------------------------------------------
def main():
    now = jst_now()
    ai_articles     = fetch_top5(AI_FEEDS,     KEYWORDS_AI,     now)
    crypto_articles = fetch_top5(CRYPTO_FEEDS, KEYWORDS_CRYPTO, now)

    today    = now.strftime("%Y-%m-%d")
    filename = f"news_{now.strftime('%Y%m%d')}.md"

    md = [f"# {today} ã® AIãƒ»ä»®æƒ³é€šè²¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚\n"]
    md += build_section(ai_articles,     "ğŸ”· AI ãƒ‹ãƒ¥ãƒ¼ã‚¹")
    md += build_section(crypto_articles, "ğŸ”¶ ä»®æƒ³é€šè²¨ãƒ‹ãƒ¥ãƒ¼ã‚¹")

    with open(os.path.join(SAVE_DIR, filename), "w", encoding="utf-8") as f:
        f.write("\n".join(md))

    print(f"âœ… ä¿å­˜å®Œäº†: {filename}")

# --------------------------------------------------
if __name__ == "__main__":
    main()

